{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used to remove stage direction if we don't want them\n",
    "def remove_stage_dir(text):\n",
    "    text = re.sub(\"[\\<].*?[\\>]\", \"\", text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used to remove the word \"SPEECH\" adn the number following after that in the corpus\n",
    "def remove_SPEECH(text):\n",
    "    text = re.sub(\"SPEECH \\d+\", \"\", text)\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/nico_marais/code/Liedjieboer/forward_farming/NLP/Text_data/The-Jargon-File.txt'\n",
    "text = open(path).read()\n",
    "text_into_list = text.replace('\\n', ' ').split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_into_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files in the folder (need to be txt with UTF-8 encoding)\n",
    "# chop it up in sentances (for Tokenizer)\n",
    "# for filename in os.listdir(path):\n",
    "#     text = ''.join(open(path+'/'+filename, encoding = 'cp1252', mode=\"r\").readlines())\n",
    "split_text = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', remove_stage_dir(text)) #change the function accordingly\n",
    "for chunk in split_text:\n",
    "        in_sentences.append(chunk)\n",
    "\n",
    "print(text_into_list[0:10])\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87caa80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of extracted sample\n",
    "maxlen = 20\n",
    "\n",
    "# Stride of sampling\n",
    "step = 1\n",
    "\n",
    "# This holds our samples sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the next word (as training label)\n",
    "next_word = []\n",
    "\n",
    "#use Kears Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_num_word = 10000 #max size of library\n",
    "tokenizer = Tokenizer(num_words=max_num_word)\n",
    "tokenizer.fit_on_texts(list(in_sentences))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list(in_sentences))\n",
    "\n",
    "#if the library ends up smaller then the max size, update the info\n",
    "if len(tokenizer.word_index) < max_num_word:\n",
    "    max_num_word = len(tokenizer.word_index)\n",
    "    \n",
    "print('Number of words:', max_num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5211672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stick the encoded words back together as a big sequence\n",
    "token_word = []\n",
    "for line in range (0,len(in_sentences)):\n",
    "    that_sentences = list_tokenized_train[line]\n",
    "    for i in range(0,len(that_sentences)):\n",
    "        token_word.append(that_sentences[i])\n",
    "\n",
    "#sample the sequence\n",
    "for i in range(0, len(token_word) - maxlen, step):\n",
    "    sentences.append(token_word[i: i + maxlen])\n",
    "    next_word.append(token_word[i + maxlen])\n",
    "print('Number of sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized x\n",
    "x = np.asarray(sentences).astype('float32')/max_num_word\n",
    "#one-hot encode y\n",
    "y = np.zeros((len(sentences), max_num_word), dtype=np.bool)\n",
    "for i in range (0,len(sentences)):\n",
    "    for j in range (0,maxlen):\n",
    "        y[i, next_word[j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e383f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build Keras model, using word embedding layer and LSTM then \n",
    "#output via softmax layer to give a prediction distribution\n",
    "from keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_num_word, 256, input_length=maxlen))\n",
    "model.add(layers.LSTM(256))\n",
    "model.add(layers.Dense(max_num_word, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Since our prediction are one-hot encoded, use `categorical_crossentropy` as the loss\n",
    "# optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.fit(x, y, batch_size=256, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c758833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for sampling the next work with a prediction distribution\n",
    "def sample(preds, temperature=0.1):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    exp_preds = preds - np.exp(temperature)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "#to change back to word\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39200ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize a seed\n",
    "import random\n",
    "\n",
    "start_index = random.randint(0, len(token_word) - maxlen - 1)\n",
    "generated_seed = token_word[start_index: start_index + maxlen]\n",
    "\n",
    "generated_text = ' '.join([reverse_word_map.get(i) for i in generated_seed])\n",
    "print('--- Generating with seed ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')\n",
    "\n",
    "for i in range(40): #generate 20 words\n",
    "\n",
    "    array_seed = np.zeros((maxlen,1))\n",
    "    array_seed[:,0] = np.asarray(generated_seed).astype('float32')/max_num_word\n",
    "    \n",
    "    preds = model.predict(array_seed.transpose(), verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_word = reverse_word_map.get(next_index)\n",
    "\n",
    "    generated_seed.append(next_index)       \n",
    "    generated_seed = generated_seed[1:]\n",
    "    generated_text = generated_text + ' ' + next_word\n",
    "\n",
    "print('--- Generated text ---')\n",
    "print(generated_text)\n",
    "print('--- --- --- --- --- ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2e6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
